{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chlorophyll Background\n",
    "\n",
    "\n",
    "Notebook on behind-the-scenes construction of `chlorophyll.ipynb`. \n",
    "\n",
    "\n",
    "I may refer to and abbreviate the **Python Data Science Handbook** by Jake VanderPlas using 'PDSH', \n",
    "possibly followed by a page number or other qualifier. \n",
    "\n",
    "\n",
    "Some content herein is broadly applicable to the entire project. \n",
    "\n",
    "\n",
    "\n",
    "## Time\n",
    "\n",
    "\n",
    "See PDSH-189. There are two time mechanisms in play: Python's built-in `datetime` and an improvement called\n",
    "`datetime64` from **numpy** that enables *arrays* of dates, i.e. time series. \n",
    "\n",
    "\n",
    "Consider these two ways of stipulating time slice arguments for `.sel()` applied to a DataSet.\n",
    "First:  Use a datetime64 with precision to minutes (or finer).\n",
    "Second: Pass strings that are interpreted as days, inclusive. In pseudo-code: \n",
    "\n",
    "```\n",
    "if do_precision:  \n",
    "   t0 = dt64('2019-06-01T00:00')\n",
    "   t1 = dt64('2019-06-01T05:20')\n",
    "   dss = ds.sel(time=slice(t0, t1))   \n",
    "else:\n",
    "    day1 = '24'\n",
    "    day2 = '27'              # will be 'day 27 inclusive' giving four days of results\n",
    "    dss = ds.sel(time=slice('2019-06-' + day1, '2019-08-' + day2))\n",
    "\n",
    "len(dss.time)\n",
    "```\n",
    "\n",
    "\n",
    "## Grid of plots\n",
    "\n",
    "Source: `data_gallery.ipynb`.  \n",
    "\n",
    "\n",
    "```\n",
    "rn = range(9); rsi = range(7)\n",
    "\n",
    "p,a=plt.subplots(3, 3, figsize=(14,14))\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "a[0,0].plot(ctdF.time, ctdF.depth, color='r');                                  a[0,0].set(ylim=(200.,0.), title='Depth')\n",
    "a[0,1].plot(ctdF.time, ctdF.salinity, color='k');                               a[0,1].set(title='Salinity')\n",
    "a[0,2].plot(ctdF.time, ctdF.temperature, color='b');                            a[0,2].set(title='Temperature')\n",
    "a[1,0].plot(ctdF.time, ctdF.dissolved_oxygen, color='b');                       a[1,0].set(title='Dissolved Oxygen')\n",
    "a[1,1].scatter(phF.time.values, phF.ph_seawater.values, color='r');             a[1,1].set(title='pH')\n",
    "a[1,2].scatter(nitrateF.time.values, nitrateF.scn.values, color='k');           a[1,2].set(title='Nitrate')\n",
    "a[2,0].plot(parF.time, parF.par_counts_output, color='k');                      a[2,0].set(title='Photosynthetic Light')\n",
    "a[2,1].plot(fluorF.time, fluorF.fluorometric_chlorophyll_a, color='b');         a[2,1].set(title='Chlorophyll')\n",
    "a[2,2].plot(siF.time, siF.si0, color='r');                                      a[2,2].set(title='Spectral Irradiance')\n",
    "\n",
    "a[2,0].text(dt64('2017-08-21T07:30'), 155., 'local midnight', rotation=90, fontsize=15, color='blue', fontweight='bold')\n",
    "a[2,2].text(dt64('2017-08-21T07:30'), 4.25, 'local midnight', rotation=90, fontsize=15, color='blue', fontweight='bold')\n",
    "\n",
    "tFmt   = mdates.DateFormatter(\"%H\")                 # an extended format for strftime() is \"%d/%m/%y %H:%M\"\n",
    "t0, t1 = ctdF.time[0].values, ctdF.time[-1].values  # establish same time range for each chart\n",
    "tticks = [dt64('2017-08-21T06:00'), dt64('2017-08-21T12:00'), dt64('2017-08-21T18:00')]\n",
    "\n",
    "for i in rn: j, k = i//3, i%3; a[j, k].set(xlim=(t0, t1),xticks=tticks); a[j, k].xaxis.set_major_formatter(tFmt)\n",
    "print('')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mini-source control: Last copied 29-SEP-2020: to tilt*, chlorophyll*, rca*, argo*\n",
    "#                      last revised 09-OCT-2020\n",
    "import os, sys, time, glob\n",
    "\n",
    "from IPython.display import clear_output             # use inside loop with clear_output(wait = True) followed by print(i)\n",
    "import warnings                                      # use with warnings.filterwarnings('ignore') or 'once'\n",
    "\n",
    "home_dir = os.getenv(\"HOME\")\n",
    "this_dir = home_dir + '/chlorophyll/'\n",
    "data_dir = '/data/'\n",
    "data1_dir = '/data1'\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors as mplcolors\n",
    "import numpy as np, pandas as pd, xarray as xr\n",
    "from numpy import datetime64 as dt64, timedelta64 as td64\n",
    "\n",
    "def doy(theDatetime): return 1 + int((theDatetime - dt64(str(theDatetime)[0:4] + '-01-01')) / td64(1, 'D')) # 1, 2, .... , 365, [366]\n",
    "def dt64_from_doy(year, doy): return dt64(str(year) + '-01-01') + td64(doy-1, 'D')\n",
    "def day_of_month_to_string(d): return str(d) if d > 9 else '0' + str(d)\n",
    "\n",
    "print('\\nJupyter Notebook running Python {}'.format(sys.version_info[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two ways of stipulating time slice arguments for a Dataset .sel()\n",
    "\n",
    "- First uses a datetime64 and is precise to minutes (or finer)\n",
    "- Second passes strings that are interpreted as days, inclusive\n",
    "\n",
    "```\n",
    "if do_precision:  \n",
    "    t0 = dt64('2019-06-01T00:00')\n",
    "    t1 = dt64('2019-06-01T05:20')\n",
    "    dss = ds.sel(time=slice(t0, t1))   \n",
    "else:\n",
    "    day1 = '24'\n",
    "    day2 = '27'              # will be 'day 27 inclusive' giving four days of results\n",
    "    dss = ds.sel(time=slice('2019-06-' + day1, '2019-08-' + day2))\n",
    "len(dss.time)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectrophotometer\n",
    "\n",
    "\n",
    "Code produces one month of water column profiles.\n",
    "\n",
    "\n",
    "For any instrument one must discern: 'When is this making measurements? What is measured?  Frequency?'\n",
    "\n",
    "\n",
    "For example the SP runs on ascent at about 3.7 samples per second; whereas nitrate runs on ascent\n",
    "at about 3 samples per minute. Going from an unexamined data file to an accurate picture of how the\n",
    "sensor works is a bit of effort. \n",
    "\n",
    "\n",
    "## deconstruct data patterns\n",
    "\n",
    "Needs work...\n",
    "\n",
    "\n",
    "- Start by assigning the data to an XArray Dataset using `ds = xr.open_dataset(fnm)`. \n",
    "    - When the data are dispersed across multiple files use `ds = xr.open_mfdataset('data_with_*_.nc')`\n",
    "        - Note this uses wildcard construction \n",
    "- Examine the Dataset using `ds`\n",
    "    - Look at the time dimension\n",
    "        - It is sometimes a poor choice to use `obs` or `observation` as the dimensional coordinate\n",
    "            - This creates degeneracy over multiple files\n",
    "            - We can use `.swap_dims` to swap time in; making multiple Datasets combinable\n",
    "    - `ds.time[0].values, ds.time[-1].values` gives a span but nothing about duty cycles\n",
    "        - As an example: 2019 spectrophotometer data\n",
    "            - Oregon Slope Base\n",
    "            - 86 channels\n",
    "            - 7 million samples\n",
    "\n",
    "\n",
    ". . . some work happens leading to . . . \n",
    "\n",
    "            - Only operates during midnight and noon ascent; at 3.7 samples per second\n",
    "            - Data has frequent dropouts over calendar time\n",
    "            - Data has spikes that tend to register across all 86 channels\n",
    "            - Very poor documentation; even the SME report is cursory\n",
    "            \n",
    "\n",
    "- Next examine an individual day (two ascent profiles).\n",
    "\n",
    "\n",
    "```\n",
    "# revealing looks... isolate one to print\n",
    "# ds_ascent2.time[0], ds_ascent2.time[-1]\n",
    "# ds_ascent2.optical_absorption\n",
    "# ds_ascent2.beam_attenuation\n",
    "# ds_optaa\n",
    "```\n",
    "\n",
    "## ([`optaa`](https://oceanobservatories.org/instrument-class/optaa/)) at **Oregon Slope Base**, 2019, JAN\n",
    "\n",
    "\n",
    "<BR>\n",
    "<img src=\"./images/spectrophotometer/osb_2019_sp_availability.png\" style=\"float: left;\" alt=\"drawing\" width=\"1200\"/>\n",
    "<div style=\"clear: left\"><BR>\n",
    "    \n",
    "    \n",
    "Above: Spectrophotometer data availability at Oregon Slope Base site courtesy the Interactive Oceans data portal. \n",
    "2019 is somewhat intermittent and stops in September. The first run here will consolidate 2019 data; and later \n",
    "expand to include the full duration and all sites. \n",
    "    \n",
    "    \n",
    "### Exposition\n",
    "\n",
    "\n",
    "* Instrument duty cycle\n",
    "    * The spectrophotometer sensor located on the Oregon Slope Base shallow profiler is *not* on all the time.\n",
    "    * It runs episodically, twice per day: During the midnight and noon profile ascents (and *not* during descent)\n",
    "        * **The first profile is 7:22 Zulu: midnight off the coast of Oregon state**\n",
    "        * RCA shallow profilers execute nine profiles per day from 200m to 5m nominal depths, then back down\n",
    "        * Foreshadowing: A scientist *could* consider a single profile as a time-evolving study since the \n",
    "          rise-fall requires over an hour to complete. In this work we treat the profile as water column \n",
    "          characterization with a timestamp. In other words the operative dimension is pressure/depth, not time; \n",
    "          and profiles are then considered in coarser time series.\n",
    "    * Two of the nine daily profiles run at local midnight and noon\n",
    "        * These feature spectrophotometer measurements during *ascent* and nitrate measurements on *descent*.\n",
    "    * These two special profiles take upwards of two hours to complete. (The other seven require less time.)\n",
    "    * In between profiles: The profiler or Science Pod (SCIP) is at rest on a platform at a depth of 200 meters\n",
    "    * The ascent minimum depth is five meters but is typically more, varying with sea conditions\n",
    "* Spectrophotometer (two ascents per day, local midnight and noon as noted above)\n",
    "    * Data are optical absorption (abbreviated **OA**), beam attenuation (abbreviated **BA**), time and pressure\n",
    "        * In this work pressure in dbar and depth in meters are treated as equivalent.\n",
    "    * Instrument sampling rate is ~3.7 samples per second\n",
    "    * Instrument records 86 spectral channels\n",
    "        * Light wavelength is ~(400nm + channel number x 4nm)\n",
    "        * Channel width is ~20nm so channels overlap\n",
    "        * Signals shift with wavelength making it possible to stack profiles in a single chart\n",
    "    * Channels 0, 83, 84 and 85 tend to give `nan` values (not usable) for both OA and BA\n",
    "        * In this work we tend to use channels 2 through 82\n",
    "    * Both OA and BA data are idiosyncratic\n",
    "        * The midnight OA data are quantized in a peculiar manner; see charts below\n",
    "        * The noon OA are *somewhat* quantized but have more reasonable / data-like structure\n",
    "        * BA data are not fraught with the OA quantization issue\n",
    "            * Both midnight and noon BA data include substantial noise\n",
    "            * Variance is also apparent in BA data\n",
    "            * This suggests filtering by depth bin and possibly discarding outliers\n",
    "* Un-answered questions\n",
    "    * Why are OA data different in midnight versus noon profiles? \n",
    "    * Are OA and BA typically combined into a turbidity value?\n",
    "    * What wavelength ranges are of particular interest?\n",
    "    * How do these signals compare with fluorometers, nitrate, CTD, pH, etcetera? \n",
    "        * The OOI site has a brief SME evaluation circa 2016 that does not illuminate actual data use cases.\n",
    "* References froom OOI\n",
    "    * [Table of instruments / designators / locations](https://oceanobservatories.org/instrument-series/optaad/)\n",
    "    * [Spectrophotometer page](https://oceanobservatories.org/instrument-class/optaa/)\n",
    "    * [Subject Matter Expert evaluation](https://oceanobservatories.org/2016/07/successful-sme-evaluation-spectrophotometer-optaa/)\n",
    "    * [Code](https://github.com/oceanobservatories/ion-functions/blob/master/ion_functions/data/opt_functions.py)\n",
    "\n",
    "\n",
    "Paraphrasing the Subject Matter Export evaluation (link above): \n",
    "\n",
    "\n",
    "> Dr. Boss (SME) verified 1.5 months of data (April-May 2015): Processing and plotting data using the raw data and vendor calibration files \n",
    "> from the AC-S, salinity and temperature from a collocated CTD data to correct absorption and attenuation median spectra and scattering, \n",
    "> and data from a collocated fluorometer to cross-check the chlorophyll and POC results.\n",
    "> \n",
    "> Consistency between the sensors suggests that they did not foul during the deployment. Not only did his results show that accurate data \n",
    "> was being produced by all the sensors in question, but the AC-S (an extremely sensitive instrument normally deployed for very short periods\n",
    "> of time) did not drift noticeably during the deployment period, a notable achievement.\n",
    "\n",
    "\n",
    "\n",
    "From the [sheet on Optical Absorption (**OA**)](https://oceanobservatories.org/wp-content/uploads/2015/10/1341-00700_Data_Product_SPEC_OPTABSN_OOI.pdf):\n",
    "\n",
    "\n",
    "> The primary instrument (OPTAA) is the WET Labs ac-s spectral absorption and attenuation meter. \n",
    "The instrument provides a 75 wavelength output from approximately 400â€“750 nm with approximately \n",
    "4 nm steps. Individual filter steps have a full-width half maximum response that\n",
    "range between about 10 to 18 nm. \n",
    ">\n",
    "> There are a total of 35 OPTAA instruments deployed\n",
    "throughout the initial OOI construction and integrated into the Pioneer, Endurance, Regional and\n",
    "Global arrays. They are deployed at fixed depths (near-surface, mid-water column and sea floor)\n",
    "and installed on moored profilers.\n",
    ">\n",
    "> The ac-s performs concurrent measurements of the water attenuation and absorption \n",
    "(the latter called 'OPTABSN').\n",
    ">\n",
    "> OPTABSN is a L2 product in that computation requires the raw signals emanating from a properly\n",
    "calibrated and configured instrument as well as water temperature (TEMPWAT) and practical\n",
    "salinity (PRACSAL) derived from a co-located and synchronized CTD. \n",
    ">\n",
    "> While small corrections\n",
    "for salinity are available at visible wavelengths (< 700 nm), temperature and salinity corrections\n",
    "are more significant at infrared wavelengths (> 700 nm) and must be performed on both the\n",
    "absorption and attenuation (OPBATTN) signals.\n",
    "\n",
    "\n",
    "The [beam attenuation (**BA**) sheet](https://oceanobservatories.org/wp-content/uploads/2015/10/1341-00690_Data_Product_SPEC_OPTATTN_OOI.pdf)\n",
    "is similar. Both give a mathematical basis for the data as well as (MATLAB?) code. \n",
    "\n",
    "\n",
    "### Practical interpretation of Spectrophotometer data\n",
    "    \n",
    "    \n",
    "The data are provided as NetCDF-CF format files. The code used here is the Python `XArray` package \n",
    "built to work with this file format, specifically via the `Dataset` and \n",
    "the `DataArray` structures. These in turn inherit from the `pandas` `DataFrame` and `Series`; which in turn\n",
    "are built on the `numpy` n-dimensional array `ndarray`. All of this requires considerable time to \n",
    "learn and internalize. Some care is taken in this narrative to provide salient remarks for the Learner. \n",
    "In polemical terms I recommend two approaches for the person new to this set of tools: \n",
    "    \n",
    "    \n",
    "- Read through this notebook without too much concern for detail (suited to a focus on \n",
    "    the data, not on learning to build new workflows).\n",
    "- Work methodically through Jake VanDerplas' excellent (free, online) book \n",
    "    [_The Python Data Science Handbook_ (herein abbreviated **PDSH**)](https://jakevdp.github.io/PythonDataScienceHandbook/) \n",
    "    particularly chapters 2 and 3, to gain expertise with `NumPy` and `pandas` prior to taking on `XArray`. \n",
    "\n",
    "\n",
    "As an example of the challenge of learning `XArray`: The reduction of this data to binned profiles\n",
    "requires a non-trivial workflow. A naive approach can result in a calculation that should take \n",
    "a seconds run for hours. (A key idea of this workflow -- the sortby() step -- is found on page 137 of **PDSH**.)\n",
    "    \n",
    "    \n",
    "- `swap_dims()` to substitute `pressure` for `time` as the ordinate dimension\n",
    "- `sortby()` to make the `pressure` dimension monotonic\n",
    "- Create a pressure-bin array to guide the subsequent data reduction\n",
    "- `groupby_bins()` together with `mean()` to reduce the data to a 0.25 meter quantized profile\n",
    "- use `transpose()` to re-order wavelength and pressure, making the resulting `DataArray` simpler to plot\n",
    "- accumulate these results by day as a list of `DataArrays`\n",
    "- From this list create an `XArray Dataset`\n",
    "- Write this to a new NetCDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "#\n",
    "# Spectrophotometer\n",
    "#   OA = Optical Absorbance (deferred owing to pathologies in the data, particularly midnight)\n",
    "#   BA = Beam Absorbance\n",
    "#\n",
    "####################\n",
    "\n",
    "# single data read covers OA and BA, both ascents each day, 2019 JAN - SEP\n",
    "ds_optaa = xr.open_dataset(data_dir + 'rca/simpler/osb_sp_optaa_2019.nc')\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "include_plots = False\n",
    "\n",
    "m_strs = ['01', '02', '03', '04', '05', '06', '07', '08', '09']           # relevant 2019 months\n",
    "m_days = [31, 28, 31, 30, 31, 30, 31, 31, 30]                             # days per month in 2019\n",
    "\n",
    "month_index = 0                                                           # manage time via months and days; 0 is January\n",
    "month_str   = m_strs[month_index]  \n",
    "year_str    = '2019'\n",
    "\n",
    "n_meters          = 200\n",
    "n_bins_per_meter  = 4\n",
    "halfbin           = (1/2) * (1/n_bins_per_meter)\n",
    "n_pressure_bins   = n_meters * n_bins_per_meter\n",
    "n_wavelengths     = 86\n",
    "wavelength        = [i for i in range(n_wavelengths)]\n",
    "p_bounds          = np.linspace(0., n_meters, n_pressure_bins + 1)             # 801 bounds: 0., .25, ..., 200.                   \n",
    "pressure          = np.linspace(halfbin, n_meters - halfbin, n_pressure_bins)  # 800 centers: 0.125, ..., 199.875                  \n",
    "oa_upper_bound    = 40.\n",
    "ba_upper_bound    = 0.7\n",
    "wavelength_stride = 8\n",
    "\n",
    "ndays = m_days[month_index]\n",
    "ndayplots, dayplotdays = 3, [0, 10, 20]\n",
    "\n",
    "l_da_oa_midn, l_da_oa_noon, l_da_ba_midn, l_da_ba_noon = [], [], [], []       # these lists accumulate DataArrays by day\n",
    "\n",
    "if include_plots:\n",
    "    fig_height, fig_width, fig_n_across, fig_n_down = 6, 6, 4, ndayplots\n",
    "    fig, axs = plt.subplots(ndayplots, fig_n_across, figsize=(fig_width * fig_n_across, fig_height*fig_n_down), tight_layout=True)\n",
    "\n",
    "# for day_index in range(m_days[month_index]):                                   # loop: days of a chosen month \n",
    "for day_index in range(ndays):\n",
    "    \n",
    "    day_str  = day_of_month_to_string(day_index + 1); date_str = year_str + '-' + month_str + '-' + day_str\n",
    "    this_doy = doy(dt64(date_str))\n",
    "    clear_output(wait = True); print(\"on day\", day_str, 'i.e. doy', this_doy)\n",
    "    midn_start = date_str + 'T07:00:00'\n",
    "    midn_done  = date_str + 'T10:00:00'\n",
    "    noon_start = date_str + 'T20:00:00'\n",
    "    noon_done  = date_str + 'T23:00:00'\n",
    "\n",
    "    # pull out OA and BA for both midnight and noon ascents; and swap in pressure for time\n",
    "    ds_midn = ds_optaa.sel(time=slice(dt64(midn_start), dt64(midn_done))).swap_dims({'time':'int_ctd_pressure'})\n",
    "    ds_noon = ds_optaa.sel(time=slice(dt64(noon_start), dt64(noon_done))).swap_dims({'time':'int_ctd_pressure'})\n",
    "    \n",
    "    da_oa_midn = ds_midn.optical_absorption.expand_dims({'doy':[this_doy]})\n",
    "    da_oa_noon = ds_noon.optical_absorption.expand_dims({'doy':[this_doy]})\n",
    "    da_ba_midn = ds_midn.beam_attenuation.expand_dims({'doy':[this_doy]})\n",
    "    da_ba_noon = ds_noon.beam_attenuation.expand_dims({'doy':[this_doy]})\n",
    "    \n",
    "    del da_oa_midn['time']; del da_oa_noon['time']; del da_ba_midn['time']; del da_ba_noon['time']\n",
    "    \n",
    "    l_da_oa_midn.append(da_oa_midn.sortby('int_ctd_pressure').groupby_bins(\"int_ctd_pressure\", p_bounds, labels=pressure).mean().transpose('wavelength', 'int_ctd_pressure_bins', 'doy'))\n",
    "    l_da_oa_noon.append(da_oa_noon.sortby('int_ctd_pressure').groupby_bins(\"int_ctd_pressure\", p_bounds, labels=pressure).mean().transpose('wavelength', 'int_ctd_pressure_bins', 'doy'))\n",
    "    l_da_ba_midn.append(da_ba_midn.sortby('int_ctd_pressure').groupby_bins(\"int_ctd_pressure\", p_bounds, labels=pressure).mean().transpose('wavelength', 'int_ctd_pressure_bins', 'doy'))\n",
    "    l_da_ba_noon.append(da_ba_noon.sortby('int_ctd_pressure').groupby_bins(\"int_ctd_pressure\", p_bounds, labels=pressure).mean().transpose('wavelength', 'int_ctd_pressure_bins', 'doy'))\n",
    "    \n",
    "    if include_plots and day_index in dayplotdays:      # if this is a plotting day: Add to the chart repertoire\n",
    "        \n",
    "        dayplotindex = dayplotdays.index(day_index) \n",
    "        oa_plot_wavelength = 12\n",
    "                \n",
    "        axs[dayplotindex][0].scatter(l_da_oa_midn[-1][oa_plot_wavelength], pressure,  marker=',', s=1, color='k') \n",
    "        axs[dayplotindex][1].scatter(l_da_oa_noon[-1][oa_plot_wavelength], pressure,  marker=',', s=1, color='b') \n",
    "        axs[dayplotindex][0].set(xlim = (.0, oa_upper_bound), ylim = (200., 0.), title='OA midnight')\n",
    "        axs[dayplotindex][1].set(xlim = (.0, oa_upper_bound), ylim = (200., 0.), title='OA noon')\n",
    "\n",
    "        for chan_sel_index in range(2, 83, wavelength_stride):\n",
    "            \n",
    "            axs[dayplotindex][2].plot(l_da_ba_midn[-1][chan_sel_index], pressure,  marker='', color='r') \n",
    "            axs[dayplotindex][3].plot(l_da_ba_noon[-1][chan_sel_index], pressure,  marker='', color='g')\n",
    "            axs[dayplotindex][2].set(xlim = (.0, ba_upper_bound), ylim = (200., 0.), title='BA midnight')\n",
    "            axs[dayplotindex][3].set(xlim = (.0, ba_upper_bound), ylim = (200., 0.), title='BA noon')\n",
    "\n",
    "        # Superimpose raw to compare: OA shows quantization; BA shows noise, outliers and suspicious jumps\n",
    "        axs[dayplotindex][0].scatter(ds_midn.optical_absorption.isel(wavelength=oa_plot_wavelength), ds_midn.int_ctd_pressure, marker=',', s=1., color='r'); \n",
    "        axs[dayplotindex][1].scatter(ds_noon.optical_absorption.isel(wavelength=oa_plot_wavelength), ds_noon.int_ctd_pressure, marker=',', s=1., color='g'); \n",
    "        axs[dayplotindex][2].scatter(ds_midn.beam_attenuation.isel(wavelength = 2 + wavelength_stride), ds_midn.int_ctd_pressure, marker=',', s=1., color='b'); \n",
    "        axs[dayplotindex][3].scatter(ds_noon.beam_attenuation.isel(wavelength = 2 + wavelength_stride), ds_noon.int_ctd_pressure, marker=',', s=1., color='k'); \n",
    "\n",
    "save_figure = False\n",
    "if save_figure: fig.savefig('/home/ubuntu/savefig.png')\n",
    "\n",
    "    \n",
    "########################################\n",
    "# \n",
    "# save resulting datasets for optaa\n",
    "#\n",
    "########################################\n",
    "\n",
    "save_optaa_datasets = False\n",
    "\n",
    "if save_optaa_datasets: \n",
    "\n",
    "    ds_oa_midn = xr.concat(l_da_oa_midn, dim=\"doy\").to_dataset(name='optical_absorption')\n",
    "    ds_oa_noon = xr.concat(l_da_oa_noon, dim=\"doy\").to_dataset(name='optical_absorption')\n",
    "    ds_ba_midn = xr.concat(l_da_ba_midn, dim=\"doy\").to_dataset(name='beam_attenuation')\n",
    "    ds_ba_noon = xr.concat(l_da_ba_noon, dim=\"doy\").to_dataset(name='beam_attenuation')\n",
    "\n",
    "    ds_oa_midn.to_netcdf(\"/data1/optaa/oa_midn_2019_01.nc\")\n",
    "    ds_oa_noon.to_netcdf(\"/data1/optaa/oa_noon_2019_01.nc\")\n",
    "    ds_ba_midn.to_netcdf(\"/data1/optaa/ba_midn_2019_01.nc\")\n",
    "    ds_ba_noon.to_netcdf(\"/data1/optaa/ba_noon_2019_01.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nitrate \n",
    "\n",
    "## profile dataset builder\n",
    "\n",
    "This code follows suit the spectrophotometer. It is simpler because there is only a nitrate value \n",
    "and no wavelength channel. \n",
    "\n",
    "I kept the pressure bins the same even though the nitrate averates about 3 three samples or less per minute\n",
    "during a 70 minute ascent. That's about three meters per minute so one sample per meter. Since the \n",
    "spectrophotometer bin depth is 0.25 meters there are necessarily a lot of empty bins (bins with no data)\n",
    "for the nitrate profile. \n",
    "\n",
    "## two open issues\n",
    "\n",
    "\n",
    "A curious artifact of the situation is from a past bias: I had understood that the SCIP makes pauses \n",
    "on descent to accommodate the nitrate sensor. I may be in error but now it looks like this sensor, \n",
    "the nitrate sensor, is observing on ascent which is continuous. This leaves open the question of \n",
    "why the pauses occur on the descent. If I have that right. \n",
    "\n",
    "\n",
    "Finally there are two nitrate signals: 'samp' and 'dark'. This code addresses only 'samp' as 'dark'\n",
    "is showing nothing of interest. So this is an open issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "#\n",
    "# Nitrate\n",
    "# \n",
    "#   dims:       time\n",
    "#   coords:     time and int_ctd_pressure\n",
    "#   data array: nitrate concentration\n",
    "#\n",
    "# To do\n",
    "#   identify when the data happens\n",
    "#   verify that the 'dark' means nothing...\n",
    "# \n",
    "####################\n",
    "\n",
    "ds_n03dark = xr.open_dataset(\"/data/rca/simpler/osb_sp_nutnr_a_dark_2019.nc\")\n",
    "ds_n03samp = xr.open_dataset(\"/data/rca/simpler/osb_sp_nutnr_a_sample_2019.nc\")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "include_charts = False\n",
    "\n",
    "m_strs = ['01', '02', '03', '04', '05', '06', '07', '08', '09']           # relevant 2019 months\n",
    "m_days = [31, 28, 31, 30, 31, 30, 31, 31, 30]                             # days per month in 2019\n",
    "\n",
    "month_index = 0                                                           # manage time via months and days; 0 is January\n",
    "month_str   = m_strs[month_index]  \n",
    "year_str    = '2019'\n",
    "\n",
    "n_meters          = 200\n",
    "n_bins_per_meter  = 4\n",
    "halfbin           = (1/2) * (1/n_bins_per_meter)\n",
    "n_pressure_bins   = n_meters * n_bins_per_meter\n",
    "p_bounds          = np.linspace(0., n_meters, n_pressure_bins + 1)             # 801 bounds: 0., .25, ..., 200.                   \n",
    "pressure          = np.linspace(halfbin, n_meters - halfbin, n_pressure_bins)  # 800 centers: 0.125, ..., 199.875                  \n",
    "nc_upper_bound    = 40.\n",
    "\n",
    "ndays = m_days[month_index]\n",
    "ndayplots, dayplotdays = 10, list(range(10))\n",
    "\n",
    "l_da_nc_midn, l_da_nc_noon = [], []       # these lists accumulate DataArrays by day\n",
    "\n",
    "if include_charts:\n",
    "    fig_height, fig_width, fig_n_across, fig_n_down = 4, 4, 2, ndayplots\n",
    "    fig, axs = plt.subplots(ndayplots, fig_n_across, figsize=(fig_width * fig_n_across, fig_height * fig_n_down), tight_layout=True)\n",
    "\n",
    "for day_index in range(ndays):\n",
    "    \n",
    "    day_str  = day_of_month_to_string(day_index + 1); date_str = year_str + '-' + month_str + '-' + day_str\n",
    "    this_doy = doy(dt64(date_str))\n",
    "    clear_output(wait = True); print(\"on day\", day_str, 'i.e. doy', this_doy)\n",
    "    midn_start = date_str + 'T07:00:00'\n",
    "    midn_done  = date_str + 'T10:00:00'\n",
    "    noon_start = date_str + 'T20:00:00'\n",
    "    noon_done  = date_str + 'T23:00:00'\n",
    "\n",
    "    # pull out OA and BA for both midnight and noon ascents; and swap in pressure for time\n",
    "    ds_midn = ds_n03samp.sel(time=slice(dt64(midn_start), dt64(midn_done))).swap_dims({'time':'int_ctd_pressure'})\n",
    "    ds_noon = ds_n03samp.sel(time=slice(dt64(noon_start), dt64(noon_done))).swap_dims({'time':'int_ctd_pressure'})\n",
    "    \n",
    "    # print('pressures:', ds_midn.int_ctd_pressure.size, ds_noon.int_ctd_pressure.size, '; times:', ds_midn.time.size, ds_noon.time.size)    \n",
    "    midn = True if ds_midn.time.size > 0 else False\n",
    "    noon = True if ds_noon.time.size > 0 else False\n",
    "        \n",
    "    if midn:\n",
    "        da_nc_midn = ds_midn.nitrate_concentration.expand_dims({'doy':[this_doy]})\n",
    "        del da_nc_midn['time']\n",
    "        l_da_nc_midn.append(da_nc_midn.sortby('int_ctd_pressure').groupby_bins(\"int_ctd_pressure\", p_bounds, labels=pressure).mean().transpose('int_ctd_pressure_bins', 'doy'))\n",
    "        \n",
    "    if noon:\n",
    "        da_nc_noon = ds_noon.nitrate_concentration.expand_dims({'doy':[this_doy]})\n",
    "        del da_nc_noon['time']\n",
    "        l_da_nc_noon.append(da_nc_noon.sortby('int_ctd_pressure').groupby_bins(\"int_ctd_pressure\", p_bounds, labels=pressure).mean().transpose('int_ctd_pressure_bins', 'doy'))\n",
    "\n",
    "    if include_charts and day_index in dayplotdays:      # if this is a plotting day: Add to the chart repertoire\n",
    "\n",
    "        dayplotindex = dayplotdays.index(day_index) \n",
    "\n",
    "        if midn:\n",
    "            axs[dayplotindex][0].scatter(l_da_nc_midn[-1], pressure,  marker=',', s=2., color='r') \n",
    "            axs[dayplotindex][0].set(xlim = (.0, nc_upper_bound), ylim = (200., 0.), title='NC midnight')\n",
    "            axs[dayplotindex][0].scatter(ds_midn.nitrate_concentration, ds_midn.int_ctd_pressure, marker=',', s=1., color='b'); \n",
    "            \n",
    "        if noon:\n",
    "            axs[dayplotindex][1].scatter(l_da_nc_noon[-1], pressure,  marker=',', s=2., color='g')\n",
    "            axs[dayplotindex][1].set(xlim = (.0, nc_upper_bound), ylim = (200., 0.), title='NC noon')\n",
    "            axs[dayplotindex][1].scatter(ds_noon.nitrate_concentration, ds_noon.int_ctd_pressure, marker=',', s=1., color='k'); \n",
    "\n",
    "save_figure = False\n",
    "if save_figure: fig.savefig('/home/ubuntu/chlorophyll/images/misc/nitrate_2019_JAN_1_to_10.png')\n",
    "\n",
    "save_nitrate_profiles = False\n",
    "\n",
    "if save_nitrate_profiles: \n",
    "    ds_nc_midn = xr.concat(l_da_nc_midn, dim=\"doy\").to_dataset(name='nitrate_concentration')\n",
    "    ds_nc_noon = xr.concat(l_da_nc_noon, dim=\"doy\").to_dataset(name='nitrate_concentration')\n",
    "\n",
    "    ds_nc_midn.to_netcdf(\"/data1/nutnr/nc_midn_2019_01.nc\")\n",
    "    ds_nc_noon.to_netcdf(\"/data1/nutnr/nc_noon_2019_01.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pH profile builder\n",
    "\n",
    "`phsen` is pH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fluorescence profile builder\n",
    "\n",
    "`flort` is chlorophyll concentration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Photosynthetically Available Radiation (PAR) profile builder\n",
    "\n",
    "`parad` is sunlight available for photosynthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectral irradiance profile builder\n",
    "\n",
    "`spkir` is also sunlight, specifically ?-welling spectral irradiance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTD profile builder\n",
    "\n",
    "Here we return to 9 profiles per day: `ctdpf` is salinity, temperature and depth measured on all profiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning basics\n",
    "\n",
    "## Sandbox build up to XArray from NumPy ndarray to pandas DataFrame\n",
    "\n",
    "### numpy ndarrays \n",
    "\n",
    "* do not have row and column headers; whereas pandas DataFrames do have typed headers\n",
    "* indexing has an equivalence of `[2][0]` to `[2,0]` \n",
    "    * The latter (with comma) is the presented way in PDSH\n",
    "    * This duality does not work for DataFrames\n",
    "* has row-then-column index order...\n",
    "    * ....with three rows in `[['l','i','s','t','1'],['s','c','n','d','2'],['t','h','r','d','3']]` \n",
    "* has slice by dimension as `start:stop:step` by default `0, len (this dimension), 1` \n",
    "    * ...exception: when `step` is negative `start` and `stop` are reversed\n",
    "    * ...multi-dimensional slices separated by commas\n",
    "    \n",
    "\n",
    "### pandas DataFrames\n",
    "\n",
    "* constructor takes `data=<ndarray>` and both `index` and `columns` arguments... \n",
    "    * ...2 dimensions only: higher dimensions and they say 'use XArray'\n",
    "    * ...and switching required a `.T` transpose\n",
    "* indexing by column and row header values, separated as in `[column_header][row_header]`\n",
    "    * as this reverses order from ndarrays: Better confirm... seems to be the case\n",
    "    * skip index/columns: defaults to integers.\n",
    "    \n",
    "    \n",
    "#### problem and solution\n",
    "\n",
    "* Problem: I can process a Dataset in a matter of hours that should take seconds\n",
    "    * I have observations across 86 channels. Call that one observation with 86 values.\n",
    "    * I have both a depth and a time associated with each observation. \n",
    "    * As a result I have multiply-indexed data.\n",
    "    * `time` is the default dimension; but I am interested in sorting by depth\n",
    "        * In fact by depth bins that incorporate many observations\n",
    "    * There are 86 x 14000 data values to average into 86 x 800 bins\n",
    "        * Expected time is perhaps a few seconds\n",
    "        * Actual time using 'self-evident' methods is an hour or more\n",
    "* Solution\n",
    "    * Page 137 of PDSH begins a section on **Rearranging Multi-Indices**\n",
    "    * The first sub-heading is **Sorted and unsorted indices**\n",
    "    * The text after both of these headings is instructive so I will quote directly.\n",
    "    \n",
    "> ***Rearranging Multi-indices***<BR>\n",
    "One of the keys to working with multiply indexed data is knowing how to effectively transform the data. \n",
    "There are a number of operations that will preserve all the information in the dataset, but rearrange \n",
    "it for the purposes of various computations. [...] There are many [ways] to finely control the rearrangement\n",
    "of data between heirarchical indices and columns.\n",
    "    \n",
    "> ***Sorted and unsorted indices***<BR>\n",
    "Earlier, we briefly mentioned a caveat, but we should emphasize it more here. \n",
    "*Many of the `MultiIndex`slicing operations will fail if the index is not sorted.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ndarray from a list of lists (notice no comma delimiter):\n",
      "\n",
      " [['l' 'i' 's' 't' '1']\n",
      " ['s' 'c' 'n' 'd' '2']\n",
      " ['t' 'h' 'r' 'd' '3']] \n",
      "\n",
      "and indexing comparison: first [0][2] then [2][0]: s t\n",
      "\n",
      "and tuplesque indexing [0, 2] or [2, 0] equivalently gives: s t\n",
      "\n",
      "So ndarrays index [slow][fast] equivalent to [row][column]\n",
      "\n",
      "     col_a col_b col_c col_d col_e\n",
      "2row     l     i     s     t     1\n",
      "4row     s     c     n     d     2\n",
      "6row     t     h     r     d     3 \n",
      "\n",
      "is a DataFrame from the ndarray; so now index [\"col_c\"][\"6row\"]: r\n",
      "\n",
      "Here is a Dataframe from a transpose of the ndarray\n",
      "\n",
      "       2row 4row 6row\n",
      "col_a    l    s    t\n",
      "col_b    i    c    h\n",
      "col_c    s    n    r\n",
      "col_d    t    d    d\n",
      "col_e    1    2    3 \n",
      "\n",
      "indexing 2row then col_e: 1\n",
      "\n",
      "So the column of a DataFrame is indexed first, then the row: Reverses the sense of the 2D ndarray.\n",
      "\n",
      "Now skipping the index= argument so the row labels default to integers:\n",
      "\n",
      "  col_a col_b col_c col_d col_e\n",
      "0     l     i     s     t     1\n",
      "1     s     c     n     d     2\n",
      "2     t     h     r     d     3 \n",
      "\n",
      "...so now indexing [\"col_d\"][0]: t \n",
      "\n",
      "      0  1  2  3  4\n",
      "2row  l  i  s  t  1\n",
      "4row  s  c  n  d  2\n",
      "6row  t  h  r  d  3 \n",
      "\n",
      "having done it the other way: used index= but not columns=. Here is element [0][\"4row\"]: s\n"
     ]
    }
   ],
   "source": [
    "###################\n",
    "#\n",
    "# A micro study of ndarray to DataFrame translation\n",
    "#\n",
    "###################\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Here is an ndarray construction from a built list of lists (not used in what follows): \n",
    "# arr = np.array([range(i, i+5) for i in [2, 4, 6]])                                       # the range() runs across columns; 2 4 6 are rows\n",
    "\n",
    "# ndarray construction: Notice all list elements are of the same type (strings)\n",
    "arr = np.array([['l','i','s','t','1'],['s','c','n','d','2'],['t','h','r','d', '3']])\n",
    "\n",
    "print('\\nndarray from a list of lists (notice no comma delimiter):\\n\\n', arr, '\\n\\nand indexing comparison: first [0][2] then [2][0]:', arr[0][2], arr[2][0]) \n",
    "print('\\nand tuplesque indexing [0, 2] or [2, 0] equivalently gives:', arr[0,2], arr[2,0])\n",
    "print('\\nSo ndarrays index [slow][fast] equivalent to [row][column]\\n')\n",
    "\n",
    "rowlist=[\"2row\", \"4row\", \"6row\"]\n",
    "columnlist = [\"col_a\", \"col_b\", \"col_c\", \"col_d\", \"col_e\"]\n",
    "df = pd.DataFrame(data=arr, index=rowlist, columns=columnlist)\n",
    "\n",
    "print(df, '\\n\\nis a DataFrame from the ndarray; so now index [\"col_c\"][\"6row\"]:', df['col_c']['6row'])\n",
    "\n",
    "df = pd.DataFrame(data=arr.T, index=columnlist, columns=rowlist)\n",
    "\n",
    "print('\\nHere is a Dataframe from a transpose of the ndarray\\n\\n', df, '\\n\\nindexing 2row then col_e:', df['2row']['col_e'])\n",
    "print('\\nSo the column of a DataFrame is indexed first, then the row: Reverses the sense of the 2D ndarray.\\n')\n",
    "print('Now skipping the index= argument so the row labels default to integers:\\n')\n",
    "\n",
    "df = pd.DataFrame(data=arr, columns=columnlist)\n",
    "\n",
    "print(df, '\\n\\n...so now indexing [\"col_d\"][0]:', df['col_d'][0], '\\n')\n",
    "\n",
    "df = pd.DataFrame(data=arr, index=rowlist)\n",
    "\n",
    "print(df, '\\n\\nhaving done it the other way: used index= but not columns=. Here is element [0][\"4row\"]:', df[0]['4row'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
