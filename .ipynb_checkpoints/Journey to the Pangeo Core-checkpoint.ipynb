{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My XArray + Dask + Intake-STAC Journey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "From some encouragement I am writing out this *process* notebook: On \n",
    "learning XArray, Dask and Intake-STAC. These (and implicitly Python) are \n",
    "the Scientist-side technical core of the Pangeo toolkit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative paths for those in a hurry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "As this notebook is a narrative learning guide it does *not* always go for\n",
    "economy of presentation. Terse documentation already \n",
    "abounds but does not include much connectivity or context.\n",
    "There are also numerous tutorials available, many referenced in the \n",
    "Pangeo education pages. Before continuing here I list some\n",
    "alternative learning paths. \n",
    "\n",
    "\n",
    "If you are interested in either of the technical topics presented here \n",
    "-- ocean data science and/or remote sensing analysis of land ice motion --\n",
    "then you might skip this essay (notebook) as it presents technical \n",
    "context in a fairly leisurely manner. The \n",
    "`chlorophyll.ipynb` notebook gets down to Pythonic business fairly \n",
    "directly. This notebook is oriented towards time-series sensor data.\n",
    "A series of narrative headlines are interspersed with content cells \n",
    "that are in turn collapsed into ellipses on the left. \n",
    "Content of interest is expandable by clicking on the ellipses.  \n",
    "Re-stow an expanded cell by selecting it and clicking on the blue bar \n",
    "in the left margin. \n",
    "\n",
    "\n",
    "The `golive.ipynb` notebook (not yet in place 3/20) is a sequel of sorts \n",
    "to the `chlorophyll.ipynb` notebook in terms of learning narrative. \n",
    "It is oriented towards raster data analysis.\n",
    "\n",
    "\n",
    "As noted above the \n",
    "[Pangeo Outreach Education Training repository](https://github.com/pangeo-data/education-material)\n",
    "provides a guide to other tutorials and learning material. Much of this \n",
    "begins at \"square one\" of XArray. \n",
    "\n",
    "\n",
    "An excellent free-online resource is Jake VanderPlas' excellent\n",
    "[Python data science handbook](https://jakevdp.github.io/PythonDataScienceHandbook).\n",
    "I hasten to add the chapter subjects to further encourage this book\n",
    "as a learning resource: **IPython**, **numpy**, **pandas**, **matplotlib**,\n",
    "and **machine learning**.\n",
    "\n",
    "\n",
    "Also cited below there is a nice \n",
    "[repo](https://github.com/coecms-training/introduction_to_xarray).\n",
    "and corresponding \n",
    "[eight video YouTube series](https://youtu.be/zoB54IpofYA)\n",
    "that comprise a very direct, economical introductory tutorial on XArray."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation for this repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wrote up this material in response to two things that \n",
    "seem fairly common. First we are a community of geoscientists trying to teach ourselves\n",
    "to use this \"machinery\" (Python, numpy, XArray, ...) developed to make common tasks \n",
    "easier to do. So we tend to put together -- not unreasonably -- learning\n",
    "materials full of \"good answers and best practices\". Second, we have inherited\n",
    "out of the inertia of the past a notion that documentation is sacrosanct and \n",
    "immutable (to borrow a term from Python). Together these two pillars of the learning\n",
    "process can place the *learner-scientist* into a position of vapor lock with no clear\n",
    "path forward. What do we do when we get stuck? This can be discouraging in my view, \n",
    "particularly when each surmounted roadblock is followed by another, all of them\n",
    "serving to make data analysis feel like grinding through a never-ending succession \n",
    "of mystifying problems. Online resources like stack overflow can be immensely \n",
    "helpful when they work; but search engines are also good at finding dead ends.\n",
    "\n",
    "\n",
    "What I would wish for as a scientist is a pleasant exploration of the meaning behind\n",
    "the data I am interested in. Here I attempt to dwell upon such a pleasant process\n",
    "while occasionally illustrating some roadblock grinding. \n",
    "This brings me to my main assertion for the novice, aspiring Pangeoist: \n",
    "\n",
    "\n",
    "> Develop a working understanding of three related things (and there is no way around\n",
    "it, it will take some time to internalize): The **XArray data model**, the associated \n",
    "syntax and a dozen-or-so methods available to manipulate, subset, reduce and visualize \n",
    "your data.\n",
    "\n",
    "\n",
    "Invariably when only difficult circuitous solutions seem to solve a given problem: \n",
    "There is an easier way. If you are considering down-sampling a time-series Dataset \n",
    "using nested `for`-loops: Find the `.resample()` method and use that instead. \n",
    "\n",
    "\n",
    "The balance of this \n",
    "notebook is a context-companion for\n",
    "exploration of the `chlorophyll` and `golive` Jupyter notebooks in this repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two research topics: coastal ocean productivity and land ice motion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I work on two research challenges here, both related to\n",
    "the broad topic of biogeochemistry. The first\n",
    "begins with an ocean observatory supporting dozens \n",
    "of sensors emplaced along the sea floor and within the water column. \n",
    "The idea is to reduce data from these sensors \n",
    "to build coherent snapshots of the state \n",
    "of the ocean at three sites. These sites are \n",
    "along the continental margin in the northeast \n",
    "Pacific, off the coast of Oregon state in the US. \n",
    "The observatory -- called the Regional Cabled Array -- measures \n",
    "pH and salinity and \n",
    "temperature and chlorphyll and other parameters about \n",
    "once per second, so each sensor stream might produce 31 \n",
    "million samples per year. The Regional Cabled Array has been \n",
    "operating since 2014.\n",
    "\n",
    "\n",
    "The second research challenge addressed here\n",
    "works from a LANDSAT-derived product called **GoLive**. This is\n",
    "a time series data collection of the speeds of all the glaciers \n",
    "on earth measured \n",
    "every few days for close to two decades.\n",
    "The challenge is to scan the data for both trends and anomalies. \n",
    "We are interested in understanding and predicting how glacier transport and\n",
    "glacial extent will change in coming decades as climate response to human \n",
    "activity continues to accelerate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What drives coastal ocean productivity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Productivity` concerns the proliferation of life in a biome. In the ocean \n",
    "it is often shorthand for primary productivity by plankton\n",
    "measured in chlorophyll mass per ocean water volume at a given\n",
    "time and depth. \n",
    "\n",
    "\n",
    "Coastal / shelf waters are many times more productive compared to\n",
    "the deep ocean. Two contributing factors are thought to be freshwater run-off from land \n",
    "and upwelling of deep water off the continental shelf. For example fresh water from the \n",
    "Columbia River might carry chromophoric dissolved\n",
    "organic matter (cdom) produced by the decay of land vegetation into coastal waters.\n",
    "There it might supply important nutrients that drive up coastal ocean productivity.\n",
    "An increase in CDOM and a decrease in surface water salinity should in principle be \n",
    "observable by the Regional Cabled Array. Upwelling events should also produce\n",
    "characteristic signals in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis charter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So... where to begin? I take a retrograde approach: Start with the data and work backwards, \n",
    "in a sense, to productivity. I begin with data from a *platform* (in fact a *profiler*) \n",
    "that rises and falls through the upper 200 meters of the water column about nine times \n",
    "per day. Break that down into all of the *instruments* carrying *sensors* producing \n",
    "data *streams* and so we have a complicated task.\n",
    "\n",
    "\n",
    "There is an unfortunate but unavoidable aspect of interpreting sensor data streams: \n",
    "Sometimes the sensor reflects the state of the ocean water accurately and sometimes \n",
    "it does not. Sensors can produce degraded or meaningless data; so the data \n",
    "analysis includes measures \n",
    "to establish a degree of confidence in our conclusions. As a first step \n",
    "to this end we can consider \n",
    "[relative standard deviation](https://en.wikipedia.org/wiki/Coefficient_of_variation).\n",
    "\n",
    "\n",
    "\n",
    "The sampling rate of these sensor is about one sample per second. Some\n",
    "operate episodically while others produce continuous data streams. Because the ocean is vertically \n",
    "stratified with depth corresponding to light (available energy) the first part of the program\n",
    "is to sort and visualize sensor data streams by *depth* at fairly coarse resolution -- say \n",
    "1 to 10 meters -- by down-sampling the stream data in time. The sensors move through \n",
    "the water column slowly so aggregating by time -- say a minute of data -- and\n",
    "retaining both mean and standard deviation reduces both sensor noise and data volume\n",
    "while providing a view of sensor veracity.\n",
    "\n",
    "\n",
    "Beyond down-sampling data streams we would also like to capture transients and \n",
    "signal structure of interest, either as individual events or taken\n",
    "as statistical distributions. For example it is common to find a maximum \n",
    "chlorophyll concentration at some depth beneath the ocean surface that \n",
    "could be recorded as a *derived* time-series signal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods of debugging the process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a fragment... the idea being to illustrate reducing a problematical\n",
    "dataset in the time axis to debug \"apparent kernel hang\".\n",
    "\n",
    "\n",
    "There are two goals of the \"in flight\" process of creating the chlorophyll\n",
    "notebook. Two goals, that is, around bringing together data streams from \n",
    "multiple sensors scattered across eight or nine instruments. The first is to\n",
    "reconcile everything to the same sample metronome. The second is to\n",
    "reduce both noise and data volume by resampling the data, hopefully without\n",
    "suppressing transients of real interest. \n",
    "\n",
    "\n",
    "The two XArray methods that correspond to these tasks are `.merge()` and\n",
    "`.resample()`. But which to do first? Intuitively it would make sense to\n",
    "`resample` the data first for each Dataset. So the fluorometer would be \n",
    "resampled and the CTD would be resampled independently; and then once \n",
    "those data volumes have been reduced by a factor of 60 they could be \n",
    "easily merged, particularly having been converted to the same cadence\n",
    "of time stamps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduction by mean and standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In cases where the sensor is noisy and the sampling rate is high (for example one \n",
    "sample per second) we have the option to reduce noise by averaging. This presumes \n",
    "that the sensor depth and elapsed time change little over the averaging interval; \n",
    "suppose about one meter and one minute. \n",
    "\n",
    "\n",
    "If as often appears to be the case the data are noisy then an average value \n",
    "might mask that; so it will also be a useful idea to calculate the standard \n",
    "deviation of the averaged samples. We have a really useful resource in XArray\n",
    "to this end, the `.resample()` method which essentially *expects* a second\n",
    "method appended that guides *how* the resampling is done. We have a Dataset\n",
    "`ds` and...\n",
    "\n",
    "```\n",
    "ds_mean = ds.resample(time='1Min').mean()\n",
    "ds_std  = ds.resample(time='1Min').std()\n",
    "```\n",
    "\n",
    "The data are time-series and the `resample()` method expects a resampling \n",
    "dimension. Time is an expected pattern of course so a system of time strings\n",
    "was developed. In this case the `Min` qualifier indicates minutes so `'1Min'`\n",
    "with the trailing `.mean()` translates to \"**Resample this Dataset across \n",
    "all of its Coordinates and Data Variables to a regular series of time \n",
    "stamps: One per minute.**\"\n",
    "\n",
    "\n",
    "The second line of code acts as a data quality metric since each Data variable \n",
    "and Coordinate resampled by the standard deviation method `.std()` will have\n",
    "values indicating the relative noisiness of the data averaged over that interval. \n",
    "Temperature is fairly easy to measure consistently and we expect that will be \n",
    "reflected in the `ds_std.seawater_temperature` DataArray. In contrast the\n",
    "fluorometric chlorophyll signal in `ds_std` should be\n",
    "very noisy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter notebooks: Three desired outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "The goal here is to avoid going through the various learning curves to produce an \n",
    "end result that \"gleams with perfection\" with no hint as to how it was produced. \n",
    "Furthermore there is nothing magical about binning results in ten meter depth intervals. \n",
    "What if you want 20 meters? Or 2 meters? My arbitrary choices should not \n",
    "influence another person interested using these results.  So with the data we \n",
    "bring along machinery that pulls in the source data and builds the resultant data. \n",
    "This makes the steps reproducible; a new dataset is at least hypothetically\n",
    "easy to generate with minimal effort. There are two items of value: Data and \n",
    "machinery. The third item of value is the record of the learning process. \n",
    "\n",
    "\n",
    "> Definitive example: Each instrument has a number of sensors which produce data \n",
    "streams. The spectrophotometer produces two streams -- absorbance and attenuation --\n",
    "at a rate of about 3 samples per second. Each sample is about 80 values across the\n",
    "optical spectrum... left off here... but it would be good to point out that \n",
    "misunderstanding the timing led to incorrect use of .resample().mean() which was\n",
    "applied at regular time intervals rather than when the sensor was actually on. \n",
    "\n",
    "\n",
    "\n",
    "Python, XArray and Dask were developed to work well together and in many cases to\n",
    "speed up processing time on large multi-dimensional datasets. As the end goal is for\n",
    "the results to be openly discoverable there is a strong case for learning\n",
    "and using Intake-STAC as a last piece of the machinery; but there is a ways to go\n",
    "with the first three before arriving at that technology. \n",
    "\n",
    "\n",
    "Now what actually happened was I started with an instrument with three sensors called a \n",
    "fluorometer. The three sensors measure via fluorescence an estimate of chlorophyll in \n",
    "the water (with no way to resolve its constituency; just 'how much is there') as well as\n",
    "concentrations of color-present dissolved organic matter (cdom) and particles in general\n",
    "(via light backscatter). It took several days to figure out how to use a data-ordering\n",
    "system that takes advantage of partially built infrastructure, moving that forward to\n",
    "an interactive Python package called \"yodapy\" (for **Y**our **O**cean **Da**ta **Py**thon\n",
    "package). And the only reason I was able to use this effectively *after* several\n",
    "days of struggling to get it working is because I know and can speak with the \n",
    "Developer of code. So unless you know him too I'm at a distinct advantage, and the point\n",
    "here is that I *shouldn't* be nor do I *want* to be at an advantage. \n",
    "\n",
    "\n",
    "So now I have the data for several weeks. But I want to have the data from 2014 to 2020. \n",
    "I take as an intermediate goal just the year 2019. I can order this data but it arrives\n",
    "as multiple files; and each one is ordered not by time but by \"observation number\" with \n",
    "*time* inherently a dependent coordinate. So now I need to swap time in as the \n",
    "independent dimension/coordinate so that I can order these multiple files into a \n",
    "single Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On data provisioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    " * These notebooks describe how to get data from scratch\n",
    " * This data is kept outside of the repo but in the Pangeo working environment\n",
    " * To make this easier to access we have a **Load** mechanism\n",
    " * ...or we will once I build it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is XArray?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "XArray precedes Dask; and is built upon pandas and NumPy. \n",
    "\n",
    "\n",
    "* Why do I need it?\n",
    "* How is it going to help?\n",
    "* What does it do?\n",
    "\n",
    "\n",
    "The following steps \n",
    "require a few hours to go through; plus additional time spent internalizing\n",
    "the details, ideally by working your own examples. This is the quickest means \n",
    "I am aware of for building XArray skills. Dask is covered later.\n",
    "\n",
    "\n",
    "* Clone [this repository](https://github.com/coecms-training/introduction_to_xarray).\n",
    "* Watch and work through the accompanying [8-video YouTube tutorial](https://youtu.be/zoB54IpofYA)\n",
    "* For backing skills with pandas: Work through chapter 3 of the [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Dask?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Dask is a task scheduler that coordinates and speeds up larger computations. Some of what Dask\n",
    "is good at happens \"behind the scenes\" in XAarray; so in principle there is nothing to learn\n",
    "per se. However this is a bit vague so let's look at it from a more open-ended inquiry: What\n",
    "is going on with Dask? \n",
    "\n",
    "\n",
    "* Why do I need help beyond XArray (in memory / out of memory stuff)\n",
    "* How is Dask going to address this?\n",
    "* What does Dask do? \n",
    "\n",
    "\n",
    "Also need a section on when to know about / care about Dask, and when Dask just gets stuff done. \n",
    "Notice that Dask sometimes works behind the scenes and that the Dask tutorial referenced below \n",
    "spends a lot of calories on timing alternative code that does the same thing...\n",
    "\n",
    "\n",
    "Approach: From a (possibly Pangeo) Jupyter Lab environment clone \n",
    "[the dask tutorial repo](https://github.com/dask/dask-tutorial).\n",
    "\n",
    "\n",
    "The [YouTube workshop video from 2018](https://youtu.be/mqdglv9GnM8) runs through this tutorial. \n",
    "\n",
    "\n",
    "Unfortunately some questions are inaudible so it can be difficult to follow in places. \n",
    "Also there is very little motivation in the exposition. One key idea that goes by \n",
    "rather quickly is \"in memory / not in memory\". This refers to whether a given calculation\n",
    "fits in the computer's RAM. If not it may be a good candidate for Dask; which has a \n",
    "formalism for breaking tasks into components and executing them in an ordered fashion\n",
    "on whatever parallel resources are available. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Intake-STAC?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "* Why?\n",
    "* How?\n",
    "* What? \n",
    "\n",
    "Anticipating where thsi is going..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XArray spadework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "I have two *actual research* objectives that should ideally result in papers. I'll present these as \n",
    "short abstracts. \n",
    "\n",
    "* Temperate glaciers are thinning and receding. They also surge episodically, essentially \n",
    "decoupling from the glacial bed and moving quickly. We have global remote sensing observations\n",
    "of glaciers back to 1991 (and earlier) available. This work characterizes quiescent glacier\n",
    "behavior and capture surge events over a thirty year interval.*\n",
    "\n",
    "\n",
    "* The ocean water column is observed at high resolution at three locations in the northeast\n",
    "Pacific by the Regional Cabled Array, an observatory that is a major component of Ocean Observations\n",
    "Initiative. This work characterizes means and variances of the ocean as observed by RCA sensors\n",
    "in both time and depth. It also generates a separate dataset flagging anomalies in an idealized \n",
    "smoothly varying sequence of observations with depth; often attributed to various mixing processes.*\n",
    "\n",
    "\n",
    "I refer to these respectively as the **Ice Problem** and the **Ocean Problem**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How XArray works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Begin with a data model that closely associates coordinates with data. \n",
    "To motivate this: Here are  \n",
    "[examples of Xarray in action](http://xarray.pydata.org/en/stable/examples.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "- XArray is built on two data container forms or *types*: The `Dataset` and the `DataArray`.\n",
    "  - A Dataset is comprised of one or more DataArrays\n",
    "  - I abbreviate Datasets as `ds` and DataArrays as `da`\n",
    "    - Useful: Start a variable name with <source_>, as in `glodap_`\n",
    "    - Useful: Append a variable name with <_sensor>, as in `glodap_ds_temp` \n",
    "  - Create a Dataset out of thin air... or by compounding a DataArray\n",
    "  - Create a DataArray out of thin air... or by extraction from a Dataset \n",
    "  - The Xarray formalism expands from `pandas` dataframes\n",
    "    - As noted this is taught in the [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/)\n",
    "  - An XArray `Dataset` is comprised of four subsets with standard names\n",
    "    - Dimensions, Coordinates, Data Variables and Attributes\n",
    "    - A precise understanding of all four of these is quite helpful\n",
    "    \n",
    "    \n",
    "A key documentation link is [here at `pydata.org`](http://xarray.pydata.org/en/stable/data-structures.html) \n",
    "From this documentation I transcribe: The four parts of an Xarray Dataset are\n",
    "\n",
    "1. **Dimensions** (dims)\n",
    "  - A dictionary mapping from dimension names to the fixed length of each dimension \n",
    "    - For example in dictionary notation: {'x': 6, 'y': 6, 'time': 8}\n",
    "2. **Data variables** (data_vars)\n",
    "  - A dict-like container of DataArrays corresponding to variables\n",
    "3. **Coordinates** (coords)\n",
    "  - Another dict-like container of DataArrays intended to label points used in data_vars\n",
    "    - For example arrays of numbers, datetime objects or strings\n",
    "4. **Attributes** (attrs)\n",
    "  - A dict holding arbitrary metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Above I mentioned \"running out of RAM\" as a problem: Large calculations suggest using Dask. \n",
    "Another scale aspect, the 'positive flip side of the coin', stems from focused investigation.\n",
    "A *small* data collection, perhaps 2 or 3 parameters at a specific site over a limited\n",
    "time range can take a great deal of time and effort. However these are increasingly expanding\n",
    "out to much larger datasets via satellite proxies, deployable sensors with high sample rates\n",
    "and other such innovations. One hopes to generalize a\n",
    "specific result to a larger study. \n",
    "\n",
    "\n",
    "While the data acquisition might scale upwards the task of painstakingly cleaning up data\n",
    "for analysis might 'come along for the ride' creating a time bottleneck. This is motivates\n",
    "the kind of work we find in geospatial data handling projects such as Xarray and Dask. \n",
    "\n",
    "\n",
    "Returning for a moment to our two practical examples... the Ice and Ocean problems.\n",
    "\n",
    "\n",
    "The Ice Problem was developed in a few-degrees-square region of Southeast Alaska \n",
    "(a single UTM zone) with a lot of moving ice. The method however applies to the Himalayas, \n",
    "to the Patagonian Icefield, to British Columbia and to many other glacier-covered regions. \n",
    "The Ice Problem computation ought to run on a global scale over the \n",
    "full time extent of the available data, in excess of a decade, from 'a single keystroke'.\n",
    "Albeit after a few preliminary keystrokes. \n",
    "\n",
    "\n",
    "The Ocean problem grows in scale according to this progression:\n",
    "\n",
    "\n",
    "- There is one Regional Cabled Array under consideration\n",
    "  - Note there are others, e.g. Neptune Canada, but let's stay 'small' for the moment\n",
    "- This Array has three profiling sites: Axial Base, Oregon Slope Base, Oregon Offshore\n",
    "- Each Profiling Sites has three instrument platforms: Deep profiler, shallow profiler, shallow platform\n",
    "- Each platform carries multiple sensor packages, each generating one or more data streams\n",
    "  - CTD\n",
    "    - Time, Pressure, Temperature, Salinity, Dissolved Oxygen\n",
    "  - Fluorometer\n",
    "  - PAR\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XArray subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "XArray subsetting can be confusing. The first step is to use the `sel()` convenience method \n",
    "with a slice for parameter range. This operates on **dimensions** (as does `isel()`) leading\n",
    "to a potential source of confusion. So the following remark is offset for emphasis: \n",
    "\n",
    "\n",
    "> Use `sel()` and slices to subset data by a dimension (with its corresponding like-named\n",
    "coordinate). However `sel()` is *not* usable to filter on non-dimensional `Coordinates` or\n",
    "on `Data variables`. These are filterable using the `where()` method. \n",
    "\n",
    "\n",
    "For the Ocean Problem we would like -- for a given time range and site and profiler \n",
    "platform --  a reconciliation of as many as *nine* instruments, each with one or more \n",
    "sensor streams. And by reconciliation we mean that it handles various sampling rates \n",
    "gracefully. Features of this reconciliation include:\n",
    "\n",
    "\n",
    "* An XArray Dataset\n",
    "  * Dimensions that accommodate typical \"one sample per second\" instruments\n",
    "  * Dimensions that accommodate slower sampling rates (pH, nitrate)\n",
    "  * Dimensions of lat and lon to make location accessible\n",
    "  * Dimension of pressure in dbar corresponding roughly to depth in meters\n",
    "* An aggregated XArray Dataset\n",
    "  * In n-day blocks (n = 1, 2, 3, ..., 7, 8) over a year x m vertical blocks (say 10 meter depth intervals)\n",
    "  * For each sensor data stream: Mean, standard deviation, number of samples, depth, center time\n",
    "  * Handle missing data gracefully via np.nan values\n",
    "* Second aggregated XArray Dataset\n",
    "  * As above with time of day also factored in in relation to daylight\n",
    "* Annotation dataset\n",
    "  * Coordinate: By date and profile\n",
    "  * Presence / Absence\n",
    "    * Inversion signals\n",
    "    * \"thin layer\"\n",
    "    * \"out of bounds\" signal\n",
    "\n",
    "\n",
    "```\n",
    "time0 = dt64('2019-06-23T00')            # a known good start time\n",
    "time1 = time0 + td64(20, 'h')            # 20 hours later; a good time range\n",
    "\n",
    "rca_subds_chlor = rca_ds_chlor.sel(time = slice(time0, time1))\n",
    "rca_subds_chlor_pressure = rca_subds_chlor.sel(int_ctd_pressure = slice(0., 40.))\n",
    "rca_subds_chlor_pressure\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streamlining the source data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Want to describe the idea of throwing everything out: Unnecessary except...\n",
    "\n",
    "* It might be of psychological value to have a reduced-scope dataset\n",
    "* It might be easier to read the content of the dataset and write new code\n",
    "* It might reduce the data volume if that resource is constrained\n",
    "* It might??? make code run faster to have less stuff to lseek() past???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A digression on the journey to `where()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "I had some severe confusion on a really basic aspect of XArray, alluded to above. I had \n",
    "an xarray Dataset from a marine profiler changing its depth with time while generating sensor values. \n",
    "\n",
    "\n",
    "* Let the `Data variable` be `PAR` that varies with `Dimension time`\n",
    "  * I want to subset the `PAR` data based on a time range and a depth range\n",
    "* There is a `pressure` (i.e. depth) `Coordinate` that varies with `time`\n",
    "  * This is called a `Coordinate without dimension` \n",
    "* `time` is a `Dimension` and also a `Coordinate` per NetCDF-CF convention. \n",
    "  * In an XArray Dataset printout this fact is indicated by an asterisk next to the `time Coordinate`\n",
    "* Subsetting the data by time range works fine:\n",
    "  * `small = ds.sel(time=slice(t0,t1))`\n",
    "* Subsetting this further to a depth range ***does not work using `sel()`***\n",
    "\n",
    "\n",
    "Eventually I realized that `.where()` does the right sort of filtering by `Coordinate`. This distinction\n",
    "is not immediately apparent in the native documentation. Solution: \n",
    "\n",
    "```\n",
    "smaller = small.where(small.depth < 60.)\n",
    "smaller2 = smaller.where(smaller.depth > 40.)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How `resample()` was mis-applied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "This covers the spectrophotometer *instrument*. It measures both \n",
    "*absorbance* **a** and *attenuation* **c** across about 86 spectral\n",
    "channels (wavelengths). It does this about three times per second.\n",
    "\n",
    "\n",
    "`ds_optaa` is the name of the spectrophotometer Dataset spanning 2019. \n",
    "It is considerably reduced from the original data having gone through \n",
    "the streamlining step described above. \n",
    "\n",
    "\n",
    "<BR>\n",
    "<img src=\"./rca_optaa0.png\" style=\"float: left;\" alt=\"drawing\" width=\"700\"/>\n",
    "<div style=\"clear: left\"><BR>\n",
    "\n",
    "\n",
    "Guessing one day of data might be 86400 samples I created a subset Dataset `ds`\n",
    "from the start of this long Dataset. I did this using the index-based selector \n",
    "method `isel()`. That is: I just wanted the first 86400 values along the time \n",
    "dimension which is an index concept. This suggests the *index-selector* `isel()`.\n",
    "    \n",
    "The `isel()` call directs a slice of indices along the time dimension. There is\n",
    "another dimension `wavelength` and this comes along in its entirety without mention.\n",
    "By default XArray must be inclusive.\n",
    "\n",
    "<BR>\n",
    "<img src=\"./rca_optaa1.png\" style=\"float: left;\" alt=\"drawing\" width=\"500\"/>\n",
    "<div style=\"clear: left\"><BR>\n",
    "\n",
    "<BR>\n",
    "<img src=\"./rca_optaa2.png\" style=\"float: left;\" alt=\"drawing\" width=\"400\"/>\n",
    "<div style=\"clear: left\"><BR>\n",
    "\n",
    "<BR>\n",
    "<img src=\"./rca_optaa3.png\" style=\"float: left;\" alt=\"drawing\" width=\"400\"/>\n",
    "<div style=\"clear: left\"><BR>\n",
    "\n",
    "<BR>\n",
    "<img src=\"./rca_optaa4.png\" style=\"float: left;\" alt=\"drawing\" width=\"400\"/>\n",
    "<div style=\"clear: left\"><BR>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dask narrative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "The first thing they try to teach us about Dask is that it has a method -- really a *decorator* -- that operates on a computational task\n",
    "in two phases. The first phase is where dask draws a graph of the problem; and the second phase is where dask grabs execution threads \n",
    "made available by the host computer and uses each of them to resolve the nodes of this graph which are of course smaller compute tasks\n",
    "that must be run in some implicit order. This implies there must be something very clever about dask that allows it to construct this\n",
    "directed acyclic *task solver* graph... but I suspect that the cleverness resides with us as coders. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask `delayed`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "We begin by using functions that have built in one-second delays that simulate some computing time. The do trivial things. \n",
    "The functions are themselves not touched by the dask formalism; but the composition of these functions into a compute task\n",
    "brings in the dask function `delayed`.\n",
    "\n",
    "\n",
    "I learn that `dask.delayed` is a Python *decorator* so here is what that means:\n",
    "\n",
    "\n",
    "> A decorator is a design pattern in Python that allows a user to add new functionality to an \n",
    "existing object without modifying its structure. Decorators are usually called before the \n",
    "definition of a function you want to decorate. [...] **Functions in Python [...] support operations \n",
    "such as being passed as an argument, returned from a function, modified, and assigned \n",
    "to a variable.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Need graphviz to see the graphs...\n",
    "\n",
    "```\n",
    "conda install graphviz\n",
    "```\n",
    "\n",
    "\n",
    "and then as it still seemed to be non-working...\n",
    "\n",
    "\n",
    "```\n",
    "pip install graphviz\n",
    "```\n",
    "\n",
    "It *seemed* like both were necessary but that seems odd... maybe just the `conda install` is all that was needed. Anyway now I have graphs that illustrate dask's thinking. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impressions of `dask.delayed`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "To understand the second and third examples I'm matching `delayed` mentally to any compute-heavy task.\n",
    "Here that means anything with a built-in `sleep(1)` to mimic a lot of work. So write out sequential code\n",
    "and stick `delayed(xxx)` around any slow `xxx()`. That's the recipe but it misses the implicit finesse \n",
    "from the narrative. I think this is 'the graph builds *instantaneously* and then executes *later* (\"when needed\")\n",
    "via parallel resources'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
