{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Dask and XArray Journey\n",
    "\n",
    "\n",
    "From some encouragement I am writing out impressions on learning dask and xarray; \n",
    "essential elements together with Intake-STAC of the Pangeo stack.\n",
    "\n",
    "\n",
    "## Get Going With XArray\n",
    "\n",
    "\n",
    "XArray precedes Dask; and is built upon pandas and NumPy. The following steps \n",
    "require a few hours to go through; plus additional time spent internalizing\n",
    "the details, ideally by working your own examples. This is the quickest means \n",
    "I am aware of for building XArray skills. Dask is covered later.\n",
    "\n",
    "\n",
    "* Clone [this repository](https://github.com/coecms-training/introduction_to_xarray).\n",
    "* Watch and work through the accompanying [8-video YouTube tutorial](https://youtu.be/zoB54IpofYA)\n",
    "* For backing skills with pandas: Work through chapter 3 of the [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/)\n",
    "\n",
    "\n",
    "## Dask\n",
    "\n",
    "\n",
    "Dask is a task scheduler that coordinates and speeds up larger computations. Some of what Dask\n",
    "is good at happens \"behind the scenes\" in XAarray; so in principle there is nothing to learn\n",
    "per se. However this is a bit vague so let's look at it from a more open-ended inquiry: What\n",
    "is going on with Dask? \n",
    "\n",
    "\n",
    "Approach: From a (possibly Pangeo) Jupyter Lab environment clone \n",
    "[the dask tutorial repo](https://github.com/dask/dask-tutorial).\n",
    "\n",
    "\n",
    "The [YouTube workshop video from 2018](https://youtu.be/mqdglv9GnM8) runs through this tutorial. \n",
    "\n",
    "\n",
    "Unfortunately some questions are inaudible so it can be difficult to follow in places. \n",
    "Also there is very little motivation in the exposition. One key idea that goes by \n",
    "rather quickly is \"in memory / not in memory\". This refers to whether a given calculation\n",
    "fits in the computer's RAM. If not it may be a good candidate for Dask; which has a \n",
    "formalism for breaking tasks into components and executing them in an ordered fashion\n",
    "on whatever parallel resources are available. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XArray spadework\n",
    "\n",
    "I have two *actual research* objectives that should ideally result in papers. I'll present these as \n",
    "short abstracts. \n",
    "\n",
    "* Temperate glaciers are thinning and receding. They also surge episodically, essentially \n",
    "decoupling from the glacial bed and moving quickly. We have global remote sensing observations\n",
    "of glaciers back to 1991 (and earlier) available. This work characterizes quiescent glacier\n",
    "behavior and capture surge events over a thirty year interval.*\n",
    "\n",
    "\n",
    "* The ocean water column is observed at high resolution at three locations in the northeast\n",
    "Pacific by the Regional Cabled Array, an observatory that is a major component of Ocean Observations\n",
    "Initiative. This work characterizes means and variances of the ocean as observed by RCA sensors\n",
    "in both time and depth. It also generates a separate dataset flagging anomalies in an idealized \n",
    "smoothly varying sequence of observations with depth; often attributed to various mixing processes.*\n",
    "\n",
    "\n",
    "I refer to these respectively as the **Ice Problem** and the **Ocean Problem**. \n",
    "\n",
    "\n",
    "## How XArray works\n",
    "\n",
    "Begin with a data model that closely associates coordinates with data. \n",
    "To motivate this: Here are  \n",
    "[examples of Xarray in action](http://xarray.pydata.org/en/stable/examples.html). \n",
    "\n",
    "\n",
    "### Model\n",
    "\n",
    "\n",
    "- XArray is built on two data container forms or *types*: The `Dataset` and the `DataArray`.\n",
    "  - A Dataset is comprised of one or more DataArrays\n",
    "  - I abbreviate Datasets as `ds` and DataArrays as `da`\n",
    "    - Useful: Start a variable name with <source_>, as in `glodap_`\n",
    "    - Useful: Append a variable name with <_sensor>, as in `glodap_ds_temp` \n",
    "  - Create a Dataset out of thin air... or by compounding a DataArray\n",
    "  - Create a DataArray out of thin air... or by extraction from a Dataset \n",
    "  - The Xarray formalism expands from `pandas` dataframes\n",
    "    - As noted this is taught in the [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/)\n",
    "  - An XArray `Dataset` is comprised of four subsets with standard names\n",
    "    - Dimensions, Coordinates, Data Variables and Attributes\n",
    "    - A precise understanding of all four of these is quite helpful\n",
    "    \n",
    "    \n",
    "The parts of an Xarray Dataset:\n",
    "\n",
    "1. `Dimensions`\n",
    "2. `Coordinates`\n",
    "3. `Data variables` \n",
    "4. `Attributes` is for my purposes a *dictionary* of metadata. These can be created and deleted. \n",
    "    \n",
    "\n",
    "## Motivation\n",
    "\n",
    "\n",
    "Above I mentioned \"running out of RAM\" as a problem: Large calculations suggest using Dask. \n",
    "Another scale aspect, the 'positive flip side of the coin', stems from focused investigation.\n",
    "A *small* data collection, perhaps 2 or 3 parameters at a specific site over a limited\n",
    "time range can take a great deal of time and effort. However these are increasingly expanding\n",
    "out to much larger datasets via satellite proxies, deployable sensors with high sample rates\n",
    "and other such innovations. One hopes to generalize a\n",
    "specific result to a larger study. \n",
    "\n",
    "\n",
    "While the data acquisition might scale upwards the task of painstakingly cleaning up data\n",
    "for analysis might 'come along for the ride' creating a time bottleneck. This is motivates\n",
    "the kind of work we find in geospatial data handling projects such as Xarray and Dask. \n",
    "\n",
    "\n",
    "Returning for a moment to our two practical examples... the Ice and Ocean problems.\n",
    "\n",
    "\n",
    "The Ice Problem was developed in a few-degrees-square region of Southeast Alaska \n",
    "(a single UTM zone) with a lot of moving ice. The method however applies to the Himalayas, \n",
    "to the Patagonian Icefield, to British Columbia and to many other glacier-covered regions. \n",
    "The Ice Problem computation ought to run on a global scale over the \n",
    "full time extent of the available data, in excess of a decade, from 'a single keystroke'.\n",
    "Albeit after a few preliminary keystrokes. \n",
    "\n",
    "\n",
    "The Ocean problem grows in scale according to this progression:\n",
    "\n",
    "\n",
    "- There is one Regional Cabled Array under consideration\n",
    "  - Note there are others, e.g. Neptune Canada, but let's stay 'small' for the moment\n",
    "- This Array has three profiling sites: Axial Base, Oregon Slope Base, Oregon Offshore\n",
    "- Each Profiling Sites has three instrument platforms: Deep profiler, shallow profiler, shallow platform\n",
    "- Each platform carries multiple sensor packages, each generating one or more data streams\n",
    "  - CTD\n",
    "    - Time, Pressure, Temperature, Salinity, Dissolved Oxygen\n",
    "  - Fluorometer\n",
    "  - PAR\n",
    "  \n",
    "\n",
    "\n",
    "## XArray subsets\n",
    "\n",
    "\n",
    "XArray subsetting can be confusing. The first step is to use the `sel()` convenience method \n",
    "with a slice for parameter range. This operates on **dimensions** (as does `isel()`) leading\n",
    "to a potential source of confusion. So the following remark is offset for emphasis: \n",
    "\n",
    "\n",
    "> Use `sel()` and slices to subset data by a dimension (with its corresponding like-named\n",
    "coordinate). However `sel()` is *not* usable to filter on non-dimensional `Coordinates` or\n",
    "on `Data variables`. These are filterable using the `where()` method. \n",
    "\n",
    "\n",
    "For the Ocean Problem we would like -- for a given time range and site and profiler \n",
    "platform --  a reconciliation of as many as *nine* instruments, each with one or more \n",
    "sensor streams. And by reconciliation we mean that it handles various sampling rates \n",
    "gracefully. Features of this reconciliation include:\n",
    "\n",
    "\n",
    "* An XArray Dataset\n",
    "  * Dimensions that accommodate typical \"one sample per second\" instruments\n",
    "  * Dimensions that accommodate slower sampling rates (pH, nitrate)\n",
    "  * Dimensions of lat and lon to make location accessible\n",
    "  * Dimension of pressure in dbar corresponding roughly to depth in meters\n",
    "* An aggregated XArray Dataset\n",
    "  * In n-day blocks (n = 1, 2, 3, ..., 7, 8) over a year x m vertical blocks (say 10 meter depth intervals)\n",
    "  * For each sensor data stream: Mean, standard deviation, number of samples, depth, center time\n",
    "  * Handle missing data gracefully via np.nan values\n",
    "* Second aggregated XArray Dataset\n",
    "  * As above with time of day also factored in in relation to daylight\n",
    "* Annotation dataset\n",
    "  * Coordinate: By date and profile\n",
    "  * Presence / Absence\n",
    "    * Inversion signals\n",
    "    * \"thin layer\"\n",
    "    * \"out of bounds\" signal\n",
    "\n",
    "\n",
    "```\n",
    "time0 = dt64('2019-06-23T00')            # a known good start time\n",
    "time1 = time0 + td64(20, 'h')            # 20 hours later; a good time range\n",
    "\n",
    "rca_subds_chlor = rca_ds_chlor.sel(time = slice(time0, time1))\n",
    "rca_subds_chlor_pressure = rca_subds_chlor.sel(int_ctd_pressure = slice(0., 40.))\n",
    "rca_subds_chlor_pressure\n",
    "```\n",
    "\n",
    "\n",
    "### A digression on the journey to `where()`\n",
    "\n",
    "\n",
    "I had some severe confusion on a really basic aspect of XArray, alluded to above. I had \n",
    "an xarray Dataset from a marine profiler changing its depth with time while generating sensor values. \n",
    "\n",
    "\n",
    "* Let the `Data variable` be `PAR` that varies with `Dimension time`\n",
    "  * I want to subset the `PAR` data based on a time range and a depth range\n",
    "* There is a `pressure` (i.e. depth) `Coordinate` that varies with `time`\n",
    "  * This is called a `Coordinate without dimension` \n",
    "* `time` is a `Dimension` and also a `Coordinate` per NetCDF-CF convention. \n",
    "  * In an XArray Dataset printout this fact is indicated by an asterisk next to the `time Coordinate`\n",
    "* Subsetting the data by time range works fine:\n",
    "  * `small = ds.sel(time=slice(t0,t1))`\n",
    "* Subsetting this further to a depth range ***does not work using `sel()`***\n",
    "\n",
    "\n",
    "Eventually I realized that `.where()` does the right sort of filtering by `Coordinate`. This distinction\n",
    "is not immediately apparent in the native documentation. Solution: \n",
    "\n",
    "```\n",
    "smaller = small.where(small.depth < 60.)\n",
    "smaller2 = smaller.where(smaller.depth > 40.)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dask narrative\n",
    "\n",
    "\n",
    "The first thing they try to teach us about Dask is that it has a method -- really a *decorator* -- that operates on a computational task\n",
    "in two phases. The first phase is where dask draws a graph of the problem; and the second phase is where dask grabs execution threads \n",
    "made available by the host computer and uses each of them to resolve the nodes of this graph which are of course smaller compute tasks\n",
    "that must be run in some implicit order. This implies there must be something very clever about dask that allows it to construct this\n",
    "directed acyclic *task solver* graph... but I suspect that the cleverness resides with us as coders. \n",
    "\n",
    "\n",
    "### Dask `delayed`\n",
    "\n",
    "\n",
    "We begin by using functions that have built in one-second delays that simulate some computing time. The do trivial things. \n",
    "The functions are themselves not touched by the dask formalism; but the composition of these functions into a compute task\n",
    "brings in the dask function `delayed`.\n",
    "\n",
    "\n",
    "I learn that `dask.delayed` is a Python *decorator* so here is what that means:\n",
    "\n",
    "\n",
    "> A decorator is a design pattern in Python that allows a user to add new functionality to an \n",
    "existing object without modifying its structure. Decorators are usually called before the \n",
    "definition of a function you want to decorate. [...] **Functions in Python [...] support operations \n",
    "such as being passed as an argument, returned from a function, modified, and assigned \n",
    "to a variable.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need graphviz to see the graphs...\n",
    "\n",
    "```\n",
    "conda install graphviz\n",
    "```\n",
    "\n",
    "\n",
    "and then as it still seemed to be non-working...\n",
    "\n",
    "\n",
    "```\n",
    "pip install graphviz\n",
    "```\n",
    "\n",
    "It *seemed* like both were necessary but that seems odd... maybe just the `conda install` is all that was needed. Anyway now I have graphs that illustrate dask's thinking. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impressions of `dask.delayed`\n",
    "\n",
    "To understand the second and third examples I'm matching `delayed` mentally to any compute-heavy task.\n",
    "Here that means anything with a built-in `sleep(1)` to mimic a lot of work. So write out sequential code\n",
    "and stick `delayed(xxx)` around any slow `xxx()`. That's the recipe but it misses the implicit finesse \n",
    "from the narrative. I think this is 'the graph builds *instantaneously* and then executes *later* (\"when needed\")\n",
    "via parallel resources'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
